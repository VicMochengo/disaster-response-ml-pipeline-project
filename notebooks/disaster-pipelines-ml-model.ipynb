{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "\n",
    "#nlp text processing libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#ml processing libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster_messages.db')\n",
    "df = pd.read_sql_table(\"disaster_messages.db\",  engine)\n",
    "\n",
    "# trim df to only have rows with wanted labels/tags i,e related column should only have 0 OR 1\n",
    "df = df[(df[\"related\"] == 0)|(df[\"related\"] == 1)]\n",
    "\n",
    "X = df[\"message\"]\n",
    "Y = df.drop([\"id\", \"message\", \"original\", \"genre\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    function to normalize and tokenize text i.e disaster messages received\n",
    "    \n",
    "    Inputs:\n",
    "    Pandas series containing disaster messages\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    List of words that have been processed through provided arguments\n",
    "    \"\"\"\n",
    "    \n",
    "    # converting all text to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", str(text).lower())\n",
    "    \n",
    "    \n",
    "    # tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    #lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    \n",
    "    \n",
    "    # normalize word tokens and remove stop words\n",
    "    normalizer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    normalized = [normalizer.stem(word) for word in clean_tokens if word not in stop_words]\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", MultiOutputClassifier(RandomForestClassifier())) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 42)\n",
    "\n",
    "np.random.seed(0)\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_train_array, y_pred_array, col_names):\n",
    "    \"\"\"Evalute metrics of the ML pipeline model by iterating \n",
    "    \n",
    "    Inputs:\n",
    "    y_train_array - array containing target variable real labels.\n",
    "    y_pred_array - array containing target variable predicted labels.\n",
    "    col_names - list of strings containing names for each of the y_pred_array fields.\n",
    "       \n",
    "    Outputs:\n",
    "    data_metrics - dataframe containing accuracy, precision, recall and f1 score for a given set of y_train_array and y_pred_array labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    #evaluate metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(y_train_array[:, i], y_pred_array[:, i])\n",
    "        precision = precision_score(y_train_array[:, i], y_pred_array[:, i])\n",
    "        recall = recall_score(y_train_array[:, i], y_pred_array[:, i])\n",
    "        f1 = f1_score(y_train_array[:, i], y_pred_array[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    #sav metrics\n",
    "    metrics = np.array(metrics)\n",
    "    data_metrics = pd.DataFrame(data = metrics, index = col_names, columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "      \n",
    "    return data_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.990113   0.992599  0.994522  0.993559\n",
      "request                 0.988576   0.996529  0.937092  0.965897\n",
      "offer                   0.998771   1.000000  0.739130  0.850000\n",
      "aid_related             0.984478   0.994078  0.968574  0.981160\n",
      "medical_help            0.988269   0.996221  0.854734  0.920070\n",
      "medical_products        0.991445   0.995043  0.831263  0.905809\n",
      "search_and_rescue       0.993289   1.000000  0.753759  0.859593\n",
      "security                0.995441   1.000000  0.736686  0.848382\n",
      "military                0.995236   0.996337  0.856693  0.921253\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.993392   1.000000  0.896135  0.945223\n",
      "food                    0.994570   0.999043  0.952555  0.975245\n",
      "shelter                 0.992879   0.998105  0.920746  0.957866\n",
      "clothing                0.998105   1.000000  0.878289  0.935201\n",
      "money                   0.994980   1.000000  0.775229  0.873385\n",
      "missing_people          0.997029   1.000000  0.747826  0.855721\n",
      "refugees                0.994314   0.996429  0.836582  0.909535\n",
      "death                   0.993648   1.000000  0.862222  0.926014\n",
      "other_aid               0.978177   0.996818  0.839587  0.911471\n",
      "infrastructure_related  0.986886   0.999031  0.801711  0.889560\n",
      "transport               0.992726   0.998691  0.844027  0.914868\n",
      "buildings               0.992623   1.000000  0.852156  0.920177\n",
      "electricity             0.995697   1.000000  0.792593  0.884298\n",
      "tools                   0.998566   1.000000  0.764706  0.866667\n",
      "hospitals               0.997439   1.000000  0.764151  0.866310\n",
      "shops                   0.998873   1.000000  0.741176  0.851351\n",
      "aid_centers             0.996568   1.000000  0.717300  0.835381\n",
      "other_infrastructure    0.990369   0.997085  0.786207  0.879177\n",
      "weather_related         0.987757   0.996403  0.959876  0.977798\n",
      "floods                  0.990728   0.997925  0.890191  0.940985\n",
      "storm                   0.994365   0.996514  0.942826  0.968927\n",
      "fire                    0.998002   1.000000  0.814286  0.897638\n",
      "earthquake              0.995646   0.992188  0.961601  0.976655\n",
      "cold                    0.997387   0.997067  0.871795  0.930233\n",
      "other_weather           0.988884   0.997599  0.794455  0.884513\n",
      "direct_report           0.980790   0.995054  0.905241  0.948025\n"
     ]
    }
   ],
   "source": [
    "#calculate evaluation metrics for training set\n",
    "Y_train_pred = pipeline.predict(X_train)\n",
    "col_names = list(Y.columns.values)\n",
    "\n",
    "print(evaluate_metrics(np.array(Y_train), Y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.807284   0.840607  0.920616  0.878794\n",
      "request                 0.880436   0.753894  0.438406  0.554410\n",
      "offer                   0.996004   0.000000  0.000000  0.000000\n",
      "aid_related             0.753035   0.756606  0.601326  0.670088\n",
      "medical_help            0.920086   0.671875  0.079336  0.141914\n",
      "medical_products        0.949746   0.833333  0.072046  0.132626\n",
      "search_and_rescue       0.971262   0.666667  0.052083  0.096618\n",
      "security                0.979407   0.000000  0.000000  0.000000\n",
      "military                0.965729   0.562500  0.040000  0.074689\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.948978   0.845070  0.279070  0.419580\n",
      "food                    0.934378   0.839286  0.514364  0.637829\n",
      "shelter                 0.932995   0.806818  0.356187  0.494200\n",
      "clothing                0.986322   0.833333  0.148515  0.252101\n",
      "money                   0.975411   0.785714  0.065476  0.120879\n",
      "missing_people          0.989396   0.000000  0.000000  0.000000\n",
      "refugees                0.969110   0.888889  0.038462  0.073733\n",
      "death                   0.960197   0.830189  0.149660  0.253602\n",
      "other_aid               0.872445   0.530303  0.041966  0.077778\n",
      "infrastructure_related  0.934532   0.181818  0.004773  0.009302\n",
      "transport               0.957584   0.661538  0.144781  0.237569\n",
      "buildings               0.952974   0.804598  0.194986  0.313901\n",
      "electricity             0.980790   1.000000  0.015748  0.031008\n",
      "tools                   0.993392   0.000000  0.000000  0.000000\n",
      "hospitals               0.988935   0.000000  0.000000  0.000000\n",
      "shops                   0.994621   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988935   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.956508   0.000000  0.000000  0.000000\n",
      "weather_related         0.860765   0.833824  0.625138  0.714556\n",
      "floods                  0.956047   0.900000  0.522472  0.661137\n",
      "storm                   0.928231   0.783394  0.347756  0.481687\n",
      "fire                    0.989396   1.000000  0.041667  0.080000\n",
      "earthquake              0.966651   0.888224  0.734323  0.803975\n",
      "cold                    0.981405   0.851852  0.164286  0.275449\n",
      "other_weather           0.948363   0.350000  0.021212  0.040000\n",
      "direct_report           0.839711   0.735185  0.306091  0.432226\n"
     ]
    }
   ],
   "source": [
    "#calculate evaluation metrics for test set\n",
    "Y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "eval_metrics_1 = evaluate_metrics(np.array(Y_test), Y_test_pred, col_names)\n",
    "print(eval_metrics_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   0.764792\n",
       "request                   0.171892\n",
       "offer                     0.004534\n",
       "aid_related               0.417243\n",
       "medical_help              0.080068\n",
       "medical_products          0.050446\n",
       "search_and_rescue         0.027816\n",
       "security                  0.018096\n",
       "military                  0.033041\n",
       "child_alone               0.000000\n",
       "water                     0.064239\n",
       "food                      0.112302\n",
       "shelter                   0.088904\n",
       "clothing                  0.015560\n",
       "money                     0.023206\n",
       "missing_people            0.011449\n",
       "refugees                  0.033618\n",
       "death                     0.045874\n",
       "other_aid                 0.132396\n",
       "infrastructure_related    0.065506\n",
       "transport                 0.046143\n",
       "buildings                 0.051214\n",
       "electricity               0.020440\n",
       "tools                     0.006109\n",
       "hospitals                 0.010873\n",
       "shops                     0.004610\n",
       "aid_centers               0.011872\n",
       "other_infrastructure      0.044222\n",
       "weather_related           0.280352\n",
       "floods                    0.082795\n",
       "storm                     0.093860\n",
       "fire                      0.010834\n",
       "earthquake                0.094321\n",
       "cold                      0.020363\n",
       "other_weather             0.052866\n",
       "direct_report             0.194982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the proportion of each column that have label == 1\n",
    "Y.sum()/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metric for use in grid search scoring object\n",
    "#def perform_metric(Y_T, Y_P):\n",
    "def performance_metrics_grid_search(y_train_array, y_pred_array):\n",
    "    \"\"\"Median F1 score for all classifiers\n",
    "    \n",
    "    Inputs:\n",
    "    y_train_array - array containing target variable real labels.\n",
    "    y_pred_array - array containing target variable predicted labels.\n",
    "        \n",
    "    Outputs:\n",
    "    median F1 score for all  classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(y_pred_array)[1]):\n",
    "        f1 = f1_score(np.array(y_train_array)[:, i], y_pred_array[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.0940154157876047, total=  57.3s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.11981408731256571, total=  56.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.11801242236024845, total=  57.2s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.12962574193179, total=  49.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  5.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.14351200493616456, total=  49.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.15662104811406227, total=  49.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.10258607652366791, total=  57.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.11204709225616563, total=  57.5s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.09956255468066491, total=  57.3s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.12847595178422247, total=  48.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.1519253003569095, total=  48.9s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.16517644875853832, total=  49.1s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.09266535274545956, total= 1.7min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.11148763460175357, total= 1.8min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.11225795297372061, total= 1.8min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.1678794178794179, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.19616961844764055, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.17980990305529876, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.08733458398823707, total= 1.8min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.13194856577645894, total= 1.8min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.1094731604155004, total= 1.8min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.19113954039327175, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.16035733844468786, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.1786490830024579, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.12442361262965743, total=  50.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.12469464801243615, total=  50.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.12465533664210157, total=  50.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.1892915642915643, total=  46.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.19860700717602364, total=  46.2s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.18956372329299268, total=  46.5s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.10565126887816112, total=  50.0s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.0984708063021316, total=  49.9s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.15245259938837918, total=  50.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.20269172543662078, total=  45.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.16140543720584594, total=  45.5s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.21006060606060606, total=  45.7s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.11477222467934542, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.10833347631384796, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.09985785358919688, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.17638697269952758, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.20973493403066684, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.19548811817597944, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.08449006428988896, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.12281351696885168, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.08846153846153845, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.19923309129477035, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.18743418743418744, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.16208666767325985, total=  47.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.14180808826112926, total=  47.8s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.17000184263865856, total=  48.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.20787111123165802, total=  45.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.1939944401840491, total=  44.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.1919092859331903, total=  45.1s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.1268842024271675, total=  47.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.1412611717974181, total=  47.3s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.15570794675272287, total=  47.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.1706591877187803, total=  44.1s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.19479853311590373, total=  43.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.20297566665984487, total=  43.8s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.04790044493882091, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.07685686178061976, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.08123092692244974, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.16405436975682278, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.19997826716893655, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.18400218400218402, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.0850940329695844, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.09015151515151515, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.12786989390762976, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.1781266142455375, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.20419011882426513, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.21177196954105781, total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 106.1min finished\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"vect__min_df\": [1, 5],\n",
    "              \"tfidf__use_idf\":[True, False],\n",
    "              \"clf__estimator__n_estimators\":[10, 25], \n",
    "              \"clf__estimator__min_samples_split\":[2, 5, 10]}\n",
    "\n",
    "\n",
    "\n",
    "scorer = make_scorer(performance_metrics_grid_search)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 5)\n",
    "\n",
    "#find the best parameters\n",
    "np.random.seed(25)\n",
    "tuned_model = cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 47.37091247,  40.35765052,  47.88753104,  39.58734155,\n",
       "         93.27661999,  76.1574587 ,  95.09358795,  74.54618979,\n",
       "         40.7350653 ,  37.02441001,  40.5435195 ,  36.20297837,\n",
       "         77.46312229,  68.23160338,  77.34641186,  65.97923716,\n",
       "         38.25211596,  35.72631788,  37.66573723,  34.67455188,\n",
       "         71.1306599 ,  64.80612954,  69.95761315,  61.9532268 ]),\n",
       " 'std_fit_time': array([ 0.29795552,  0.05069102,  0.20518094,  0.18139919,  0.26688755,\n",
       "         0.11441379,  0.16457284,  0.07016877,  0.04367415,  0.11711199,\n",
       "         0.21153493,  0.08985636,  0.29616246,  0.26580368,  0.18151832,\n",
       "         0.10682591,  0.09031901,  0.08376947,  0.01403503,  0.12815117,\n",
       "         0.05559873,  0.22587277,  0.16485241,  0.15264039]),\n",
       " 'mean_score_time': array([  9.6658833 ,   9.31509686,   9.65439153,   9.27728017,\n",
       "         11.96271165,  11.09012294,  11.88440736,  10.96895941,\n",
       "          9.64531819,   9.32572969,   9.5638454 ,   9.29296891,\n",
       "         11.95657531,  11.07313259,  11.84672586,  11.00202306,\n",
       "          9.62830329,   9.26488837,   9.5851593 ,   9.2394162 ,\n",
       "         11.935848  ,  11.04794089,  11.81863594,  10.97273835]),\n",
       " 'std_score_time': array([ 0.03465236,  0.05027019,  0.04098089,  0.05476158,  0.04826972,\n",
       "         0.0644325 ,  0.08250407,  0.04858432,  0.05606356,  0.01284866,\n",
       "         0.04564717,  0.04311194,  0.04879885,  0.07480749,  0.05074114,\n",
       "         0.02399197,  0.06434564,  0.01682877,  0.03141376,  0.03089221,\n",
       "         0.04483125,  0.03139684,  0.03746218,  0.07184868]),\n",
       " 'param_clf__estimator__min_samples_split': masked_array(data = [2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False True\n",
       "  True False False True True False False True True False False],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__min_df': masked_array(data = [1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5}],\n",
       " 'split0_test_score': array([ 0.09401542,  0.12962574,  0.10258608,  0.12847595,  0.09266535,\n",
       "         0.16787942,  0.08733458,  0.19113954,  0.12442361,  0.18929156,\n",
       "         0.10565127,  0.20269173,  0.11477222,  0.17638697,  0.08449006,\n",
       "         0.15661765,  0.16208667,  0.20787111,  0.1268842 ,  0.17065919,\n",
       "         0.04790044,  0.16405437,  0.08509403,  0.17812661]),\n",
       " 'split1_test_score': array([ 0.11981409,  0.143512  ,  0.11204709,  0.1519253 ,  0.11148763,\n",
       "         0.19616962,  0.13194857,  0.16035734,  0.12469465,  0.19860701,\n",
       "         0.09847081,  0.16140544,  0.10833348,  0.20973493,  0.12281352,\n",
       "         0.19923309,  0.14180809,  0.19399444,  0.14126117,  0.19479853,\n",
       "         0.07685686,  0.19997827,  0.09015152,  0.20419012]),\n",
       " 'split2_test_score': array([ 0.11801242,  0.15662105,  0.09956255,  0.16517645,  0.11225795,\n",
       "         0.1798099 ,  0.10947316,  0.17864908,  0.12465534,  0.18956372,\n",
       "         0.1524526 ,  0.21006061,  0.09985785,  0.19548812,  0.08846154,\n",
       "         0.18743419,  0.17000184,  0.19190929,  0.15570795,  0.20297567,\n",
       "         0.08123093,  0.18400218,  0.12786989,  0.21177197]),\n",
       " 'mean_test_score': array([ 0.11061398,  0.14325293,  0.10473191,  0.1485259 ,  0.10547031,\n",
       "         0.18128631,  0.10958544,  0.17671532,  0.1245912 ,  0.19248743,\n",
       "         0.11885822,  0.19138592,  0.10765452,  0.19387001,  0.09858837,\n",
       "         0.18109498,  0.15796553,  0.19792495,  0.14128444,  0.1894778 ,\n",
       "         0.06866274,  0.18267827,  0.10103848,  0.19802957]),\n",
       " 'std_test_score': array([ 0.01175998,  0.01102231,  0.00531785,  0.01517451,  0.00905993,\n",
       "         0.01159651,  0.01821375,  0.01264095,  0.00011958,  0.00432862,\n",
       "         0.023935  ,  0.02141179,  0.00610766,  0.01366224,  0.01720632,\n",
       "         0.01796586,  0.01187321,  0.00708433,  0.01176726,  0.01371912,\n",
       "         0.01478936,  0.01469572,  0.01908469,  0.01440988]),\n",
       " 'rank_test_score': array([17, 13, 21, 12, 20,  8, 18, 10, 15,  4, 16,  5, 19,  3, 23,  9, 11,\n",
       "         2, 14,  6, 24,  7, 22,  1], dtype=int32),\n",
       " 'split0_train_score': array([ 0.90578359,  0.91709754,  0.90612563,  0.91904398,  0.98202375,\n",
       "         0.9815503 ,  0.98194398,  0.98419384,  0.84382886,  0.84444856,\n",
       "         0.84053904,  0.81321345,  0.89848485,  0.87050648,  0.88928342,\n",
       "         0.85634921,  0.79900092,  0.73381109,  0.76445779,  0.71865762,\n",
       "         0.80517297,  0.75964504,  0.78189261,  0.77840364]),\n",
       " 'split1_train_score': array([ 0.91050125,  0.91082577,  0.91109572,  0.9052633 ,  0.98052558,\n",
       "         0.98358829,  0.98128663,  0.9844088 ,  0.85560473,  0.83258792,\n",
       "         0.8288292 ,  0.8145308 ,  0.89272052,  0.84663558,  0.86449889,\n",
       "         0.86069254,  0.7874751 ,  0.74607008,  0.76123642,  0.7331303 ,\n",
       "         0.80456824,  0.7782509 ,  0.78438351,  0.75631572]),\n",
       " 'split2_train_score': array([ 0.91149509,  0.91831162,  0.9078125 ,  0.91433055,  0.98242908,\n",
       "         0.98246423,  0.98267525,  0.98075278,  0.84582558,  0.84103896,\n",
       "         0.86180858,  0.82365869,  0.89692248,  0.87411125,  0.88338825,\n",
       "         0.85504658,  0.78900408,  0.74963127,  0.78416167,  0.7486164 ,\n",
       "         0.80956223,  0.77372424,  0.8161157 ,  0.76471633]),\n",
       " 'mean_train_score': array([ 0.90925998,  0.91541165,  0.90834462,  0.91287928,  0.98165947,\n",
       "         0.98253427,  0.98196862,  0.98311847,  0.84841972,  0.83935848,\n",
       "         0.84372561,  0.81713431,  0.89604262,  0.8637511 ,  0.87905685,\n",
       "         0.85736278,  0.7918267 ,  0.74317081,  0.76995196,  0.73346811,\n",
       "         0.80643448,  0.77054006,  0.79413061,  0.76647856]),\n",
       " 'std_train_score': array([ 0.00249144,  0.00328036,  0.00206362,  0.00571877,  0.00081868,\n",
       "         0.00083348,  0.00056717,  0.0016751 ,  0.00514555,  0.00498576,\n",
       "         0.01365102,  0.00464467,  0.00243413,  0.01219165,  0.01057163,\n",
       "         0.00241381,  0.0051112 ,  0.00677612,  0.01013348,  0.01223295,\n",
       "         0.00222539,  0.00792249,  0.01557903,  0.00910305])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search results\n",
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1980295675369535"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top mean score\n",
    "np.max(tuned_model.cv_results_[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 10,\n",
       " 'clf__estimator__n_estimators': 25,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__min_df': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parameters for top mean score\n",
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.818657   0.835776  0.947145  0.887982\n",
      "request                 0.892885   0.787836  0.504529  0.615130\n",
      "offer                   0.996004   0.000000  0.000000  0.000000\n",
      "aid_related             0.777778   0.736567  0.727340  0.731924\n",
      "medical_help            0.918549   0.558824  0.105166  0.177019\n",
      "medical_products        0.951898   0.774194  0.138329  0.234719\n",
      "search_and_rescue       0.970954   0.588235  0.052083  0.095694\n",
      "security                0.979253   0.000000  0.000000  0.000000\n",
      "military                0.967727   0.758621  0.097778  0.173228\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.958814   0.871560  0.441860  0.586420\n",
      "food                    0.944829   0.810000  0.664843  0.730278\n",
      "shelter                 0.940218   0.797721  0.468227  0.590095\n",
      "clothing                0.987859   0.750000  0.326733  0.455172\n",
      "money                   0.976026   0.833333  0.089286  0.161290\n",
      "missing_people          0.989550   0.000000  0.000000  0.000000\n",
      "refugees                0.968495   0.548387  0.081731  0.142259\n",
      "death                   0.965883   0.810345  0.319728  0.458537\n",
      "other_aid               0.873982   0.585366  0.057554  0.104803\n",
      "infrastructure_related  0.934378   0.000000  0.000000  0.000000\n",
      "transport               0.957123   0.666667  0.121212  0.205128\n",
      "buildings               0.952205   0.815789  0.172702  0.285057\n",
      "electricity             0.981712   1.000000  0.062992  0.118519\n",
      "tools                   0.993853   0.000000  0.000000  0.000000\n",
      "hospitals               0.989089   0.000000  0.000000  0.000000\n",
      "shops                   0.994621   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988935   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.955894   0.125000  0.003559  0.006920\n",
      "weather_related         0.887813   0.823004  0.761301  0.790951\n",
      "floods                  0.958353   0.894895  0.558052  0.687428\n",
      "storm                   0.947288   0.772816  0.637821  0.698859\n",
      "fire                    0.988781   0.000000  0.000000  0.000000\n",
      "earthquake              0.972337   0.888686  0.803630  0.844021\n",
      "cold                    0.980944   0.833333  0.142857  0.243902\n",
      "other_weather           0.949285   0.500000  0.039394  0.073034\n",
      "direct_report           0.850930   0.755070  0.373169  0.499484\n"
     ]
    }
   ],
   "source": [
    "#evaluating metrics for test set\n",
    "tuned_pred_test = tuned_model.predict(X_test)\n",
    "\n",
    "eval_metrics_2 = evaluate_metrics(np.array(Y_test), tuned_pred_test, col_names)\n",
    "\n",
    "print(eval_metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.944752</td>\n",
       "      <td>0.567653</td>\n",
       "      <td>0.192243</td>\n",
       "      <td>0.248879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.056639</td>\n",
       "      <td>0.366853</td>\n",
       "      <td>0.245765</td>\n",
       "      <td>0.271783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.753035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.934033</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.006977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.958890</td>\n",
       "      <td>0.755250</td>\n",
       "      <td>0.068761</td>\n",
       "      <td>0.126753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.982634</td>\n",
       "      <td>0.835189</td>\n",
       "      <td>0.316507</td>\n",
       "      <td>0.444592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920616</td>\n",
       "      <td>0.878794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  36.000000  36.000000  36.000000  36.000000\n",
       "mean    0.944752   0.567653   0.192243   0.248879\n",
       "std     0.056639   0.366853   0.245765   0.271783\n",
       "min     0.753035   0.000000   0.000000   0.000000\n",
       "25%     0.934033   0.136364   0.003580   0.006977\n",
       "50%     0.958890   0.755250   0.068761   0.126753\n",
       "75%     0.982634   0.835189   0.316507   0.444592\n",
       "max     1.000000   1.000000   0.920616   0.878794"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary of 1st model\n",
    "eval_metrics_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.948969</td>\n",
       "      <td>0.531167</td>\n",
       "      <td>0.241639</td>\n",
       "      <td>0.294385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.051442</td>\n",
       "      <td>0.364587</td>\n",
       "      <td>0.285752</td>\n",
       "      <td>0.300433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.943676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.962348</td>\n",
       "      <td>0.743284</td>\n",
       "      <td>0.101472</td>\n",
       "      <td>0.175123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.983249</td>\n",
       "      <td>0.811706</td>\n",
       "      <td>0.448452</td>\n",
       "      <td>0.587339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947145</td>\n",
       "      <td>0.887982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  36.000000  36.000000  36.000000  36.000000\n",
       "mean    0.948969   0.531167   0.241639   0.294385\n",
       "std     0.051442   0.364587   0.285752   0.300433\n",
       "min     0.777778   0.000000   0.000000   0.000000\n",
       "25%     0.943676   0.000000   0.000000   0.000000\n",
       "50%     0.962348   0.743284   0.101472   0.175123\n",
       "75%     0.983249   0.811706   0.448452   0.587339\n",
       "max     1.000000   1.000000   0.947145   0.887982"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary of tuned model\n",
    "eval_metrics_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using SVM instead \n",
    "pipeline2 = Pipeline([\n",
    "    (\"vect\", CountVectorizer(tokenizer = tokenize)),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", MultiOutputClassifier(SVC()))\n",
    "])\n",
    "\n",
    "parameters2 = {\"vect__min_df\": [5],\n",
    "              \"tfidf__use_idf\":[True],\n",
    "              \"clf__estimator__kernel\": [\"poly\"], \n",
    "              \"clf__estimator__degree\": [1, 2, 3],\n",
    "              \"clf__estimator__C\":[1, 10, 100]}\n",
    "\n",
    "cv2 = GridSearchCV(pipeline2, param_grid = parameters2, scoring = scorer, verbose = 5)\n",
    "\n",
    "#find best parameters\n",
    "np.random.seed(25)\n",
    "tuned_model2 = cv2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search results\n",
    "tuned_model2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate evaluation metrics for test set\n",
    "tuned_pred_test2 = tuned_model2.predict(X_test)\n",
    "\n",
    "eval_metrics_3 = evaluate_metrics(np.array(Y_test), tuned_pred_test2, col_names)\n",
    "\n",
    "print(eval_metrics_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle best model\n",
    "pickle.dump(tuned_model, open(\"disaster_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
